{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bkJPyHL-sM8g"
   },
   "source": [
    "Imports + initialize tpu/gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9390,
     "status": "ok",
     "timestamp": 1645843844725,
     "user": {
      "displayName": "shaun singh",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11843108871723974326"
     },
     "user_tz": 300
    },
    "id": "fg9QbDyuqKkH",
    "outputId": "54e2095a-d003-46ea-ac93-beb8c16577a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Not connected to a TPU runtime\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "assert float(tf.__version__[:3]) >= 2.3\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    print('Device:', tpu.master())\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "    GCS_PATH = 'gs://kds-730dfb73e90bcfe94bba623cbc90984df476fa507c3e44b785ea223e/'\n",
    "except:\n",
    "    print('WARNING: Not connected to a TPU runtime')\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "    GCS_PATH = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WG3WIjsNsQiI"
   },
   "source": [
    "Import Pneumonia Dataset (from google store for tpu) + set variables (batch, epoch, image size, tune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 694,
     "status": "ok",
     "timestamp": 1645843845416,
     "user": {
      "displayName": "shaun singh",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11843108871723974326"
     },
     "user_tz": 300
    },
    "id": "L6L830ucqPoT"
   },
   "outputs": [],
   "source": [
    "filenames = tf.io.gfile.glob(str(GCS_PATH + 'chest_xray/train/*/*'))\n",
    "filenames.extend(tf.io.gfile.glob(str(GCS_PATH + 'chest_xray/val/*/*')))\n",
    "train_filenames, val_filenames = train_test_split(filenames, test_size=0.2)\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "BATCH_SIZE = 16 * strategy.num_replicas_in_sync\n",
    "IMG_SIZE = 224\n",
    "EPOCHS = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qbFjmz5ssbG_"
   },
   "source": [
    "Functions for processing images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1645843845417,
     "user": {
      "displayName": "shaun singh",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11843108871723974326"
     },
     "user_tz": 300
    },
    "id": "IJmeBzfBqxFG"
   },
   "outputs": [],
   "source": [
    "def get_label(file_path):\n",
    "    # convert the path to a list of path components\n",
    "    parts = tf.strings.split(file_path, os.path.sep)\n",
    "    # The second to last is the class-directory\n",
    "    return parts[-2] == \"PNEUMONIA\"\n",
    "\n",
    "def decode_img(img):\n",
    "  # convert the compressed string to a 3D uint8 tensor\n",
    "  img = tf.image.decode_jpeg(img, channels=3)\n",
    "  # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n",
    "  img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "  # resize the image to the desired size.\n",
    "  return tf.image.resize(img, [IMG_SIZE, IMG_SIZE])\n",
    "\n",
    "def process_path(file_path):\n",
    "    label = get_label(file_path)\n",
    "    # load the raw data from the file as a string\n",
    "    img = tf.io.read_file(file_path)\n",
    "    img = decode_img(img)\n",
    "    return img, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "orzvep4Ssd-v"
   },
   "source": [
    "Use buffered prefetching so we can yield data from disk without having I/O blocking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1645843845417,
     "user": {
      "displayName": "shaun singh",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11843108871723974326"
     },
     "user_tz": 300
    },
    "id": "Mz29Y0lkrQg3"
   },
   "outputs": [],
   "source": [
    "def prepare_for_training(ds, cache=True, shuffle_buffer_size=1000):\n",
    "    # This is a small dataset, only load it once, and keep it in memory.\n",
    "    # use `.cache(filename)` to cache preprocessing work for datasets that don't\n",
    "    # fit in memory.\n",
    "    if cache:\n",
    "        if isinstance(cache, str):\n",
    "            ds = ds.cache(cache)\n",
    "        else:\n",
    "            ds = ds.cache()\n",
    "\n",
    "    ds = ds.shuffle(buffer_size=shuffle_buffer_size)\n",
    "    ds = ds.repeat()\n",
    "    ds = ds.batch(BATCH_SIZE)\n",
    "    ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_LUyt3aJtcUo"
   },
   "source": [
    "Load + Process train/test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 192,
     "status": "ok",
     "timestamp": 1645843845606,
     "user": {
      "displayName": "shaun singh",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11843108871723974326"
     },
     "user_tz": 300
    },
    "id": "Rx1ame9rvHyQ"
   },
   "outputs": [],
   "source": [
    "COUNT_NORMAL = len([filename for filename in train_filenames if \"NORMAL\" in filename])\n",
    "COUNT_PNEUMONIA = len([filename for filename in train_filenames if \"PNEUMONIA\" in filename])\n",
    "TRAIN_IMG_COUNT = tf.data.experimental.cardinality(tf.data.Dataset.from_tensor_slices(train_filenames)).numpy()\n",
    "VAL_IMG_COUNT = tf.data.experimental.cardinality(tf.data.Dataset.from_tensor_slices(val_filenames)).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 26752,
     "status": "ok",
     "timestamp": 1645843872356,
     "user": {
      "displayName": "shaun singh",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11843108871723974326"
     },
     "user_tz": 300
    },
    "id": "A-HPoHEdrSIO"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-02 18:56:54.858041: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2022-03-02 18:56:57.054990: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "test_ds = tf.data.Dataset.list_files(str(GCS_PATH + 'chest_xray/test/*/*')).map(process_path, num_parallel_calls=AUTOTUNE).batch(BATCH_SIZE)\n",
    "\n",
    "train_ds = prepare_for_training(tf.data.Dataset.from_tensor_slices(train_filenames).map(process_path, num_parallel_calls=AUTOTUNE))\n",
    "val_ds = prepare_for_training(tf.data.Dataset.from_tensor_slices(val_filenames).map(process_path, num_parallel_calls=AUTOTUNE))\n",
    "\n",
    "image_batch, label_batch = next(iter(train_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZNwp2gVYv593"
   },
   "source": [
    "Data was imbalanced, with more images classified as pneumonia than normal. Each normal image will be weighted more to balance the data as the CNN works best when the training data is balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1645843872358,
     "user": {
      "displayName": "shaun singh",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11843108871723974326"
     },
     "user_tz": 300
    },
    "id": "g80fVPL1rdzZ"
   },
   "outputs": [],
   "source": [
    "weight_for_0 = (1 / COUNT_NORMAL)*(TRAIN_IMG_COUNT)/2.0 \n",
    "weight_for_1 = (1 / COUNT_PNEUMONIA)*(TRAIN_IMG_COUNT)/2.0\n",
    "\n",
    "class_weight = {0: weight_for_0, 1: weight_for_1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BoVZTXhZwkdQ"
   },
   "source": [
    "Retrain moblenetv2 model for use with dataset (transfer learning).\n",
    "\n",
    "Since there are only two possible labels for the image, we will be using the `binary_crossentropy` loss. When we fit the model, identify the class weights. Because we are using a TPU, training will be relatively quick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 805
    },
    "executionInfo": {
     "elapsed": 175717,
     "status": "ok",
     "timestamp": 1645844048069,
     "user": {
      "displayName": "shaun singh",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11843108871723974326"
     },
     "user_tz": 300
    },
    "id": "PZMC-pzGrpHs",
    "outputId": "252d5da4-0402-4da4-b7c2-495d99bf0775"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "261/261 [==============================] - 42s 149ms/step - loss: 2.0992 - accuracy: 0.7965 - val_loss: 0.5116 - val_accuracy: 0.9375\n",
      "Epoch 2/20\n",
      "261/261 [==============================] - 32s 123ms/step - loss: 0.8322 - accuracy: 0.8946 - val_loss: 0.5613 - val_accuracy: 0.9317\n",
      "Epoch 3/20\n",
      "261/261 [==============================] - 30s 117ms/step - loss: 0.8898 - accuracy: 0.9097 - val_loss: 0.5547 - val_accuracy: 0.9462\n",
      "Epoch 4/20\n",
      "261/261 [==============================] - 31s 120ms/step - loss: 6.4173 - accuracy: 0.3815 - val_loss: 11.2572 - val_accuracy: 0.2702\n",
      "Epoch 5/20\n",
      "261/261 [==============================] - 30s 115ms/step - loss: 7.7068 - accuracy: 0.2555 - val_loss: 11.2572 - val_accuracy: 0.2702\n",
      "Epoch 6/20\n",
      "261/261 [==============================] - 30s 115ms/step - loss: 7.7142 - accuracy: 0.2548 - val_loss: 11.2721 - val_accuracy: 0.2692\n",
      "Epoch 7/20\n",
      " 49/261 [====>.........................] - ETA: 19s - loss: 7.6318 - accuracy: 0.2628"
     ]
    }
   ],
   "source": [
    "with strategy.scope():\n",
    "    base_model = tf.keras.applications.MobileNetV2(weights='imagenet', input_shape=(IMG_SIZE, IMG_SIZE, 3), include_top=False)\n",
    "    base_model.trainable = False\n",
    "    model = tf.keras.Sequential([\n",
    "      base_model,\n",
    "      tf.keras.layers.GlobalAveragePooling2D(),\n",
    "      tf.keras.layers.Dropout(0.2),\n",
    "      tf.keras.layers.Dense(1),\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer = 'adam',\n",
    "        loss = 'binary_crossentropy',\n",
    "        metrics = ['accuracy']\n",
    "    )\n",
    "    \n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    steps_per_epoch=TRAIN_IMG_COUNT // BATCH_SIZE,\n",
    "    epochs=int((EPOCHS/3)*2),\n",
    "    validation_data=val_ds,\n",
    "    validation_steps=VAL_IMG_COUNT // BATCH_SIZE,\n",
    "    class_weight=class_weight,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 117643,
     "status": "ok",
     "timestamp": 1645844165708,
     "user": {
      "displayName": "shaun singh",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11843108871723974326"
     },
     "user_tz": 300
    },
    "id": "ZhJupZ1IrtHJ",
    "outputId": "26a19b3e-d5a6-41b2-8b1f-3a2812b8fb4a",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/30\n",
      "261/261 [==============================] - 49s 184ms/step - loss: 7.6267 - accuracy: 0.7450 - val_loss: 1.0343 - val_accuracy: 0.9019\n",
      "Epoch 21/30\n",
      "261/261 [==============================] - 48s 184ms/step - loss: 7.6196 - accuracy: 0.7452 - val_loss: 1.9674 - val_accuracy: 0.8433\n",
      "Epoch 22/30\n",
      "189/261 [====================>.........] - ETA: 11s - loss: 7.5456 - accuracy: 0.7477"
     ]
    }
   ],
   "source": [
    "base_model.trainable = True\n",
    "fine_tune_at = 100\n",
    "\n",
    "# Freeze all the layers before the `fine_tune_at` layer\n",
    "for layer in base_model.layers[:fine_tune_at]:\n",
    "  layer.trainable =  False\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(1e-5),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history_fine = model.fit(\n",
    "    train_ds,\n",
    "    steps_per_epoch=TRAIN_IMG_COUNT // BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    initial_epoch=history.epoch[-1],\n",
    "    validation_data=val_ds,\n",
    "    validation_steps=VAL_IMG_COUNT // BATCH_SIZE,\n",
    "    class_weight=class_weight\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "plt.figure(figsize=(8, 8), tight_layout=True)\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([min(plt.ylim()),1])\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "acc += history_fine.history['accuracy']\n",
    "val_acc += history_fine.history['val_accuracy']\n",
    "\n",
    "plt.figure(figsize=(8, 8), tight_layout=True)\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.plot([int((EPOCHS/3)*2),int((EPOCHS/3)*2)],\n",
    "          plt.ylim(), label='Start Fine Tuning')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert model to TFLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def representative_data_gen():\n",
    "    dataset_list = tf.data.Dataset.list_files(GCS_PATH + \"chest_xray/train/*/*\")\n",
    "    for i in range(100):\n",
    "        path = next(iter(dataset_list))\n",
    "        file_bytes = tf.io.read_file(path)\n",
    "        img = tf.io.decode_jpeg(file_bytes, channels=3)\n",
    "        img = tf.cast(img, tf.float32) / 255.0 \n",
    "        img = tf.image.resize(img, [IMG_SIZE, IMG_SIZE])\n",
    "        img = tf.expand_dims(img, 0)\n",
    "        yield [img]\n",
    "        \n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_data_gen\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.uint8\n",
    "converter.inference_output_type = tf.uint8\n",
    "tflite_model_quant = converter.convert()\n",
    "\n",
    "with open('xray_mobilenetv2_quant_model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model_quant)\n",
    "    \n",
    "interpreter = tf.lite.Interpreter(model_content=tflite_model_quant)\n",
    "input_type = interpreter.get_input_details()[0]['dtype']\n",
    "output_type = interpreter.get_output_details()[0]['dtype']\n",
    "print('input: ', input_type)\n",
    "print('output: ', output_type)\n",
    "\n",
    "labels = ['PNEUMONIA', 'NORMAL']\n",
    "with open('testing/xray_labels.txt', 'w') as f:\n",
    "    for line in labels:\n",
    "        f.write(line)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile for EdgeTPU. We run this using docker & rosetta since google is annoying and refuses to compile `edgetpu-compiler` for any other platforms or architecture. Ironic that you can't compile for EdgeTPU on Google's own dev board. \n",
    "\n",
    "Run `docker build --quiet --platform linux/amd64  --tag edgetpu_compiler https://github.com/tomassams/docker-edgetpu-compiler.git` to build the container if you haven't already done so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run -it --platform linux/amd64 --rm -v (pwd):/home/edgetpu edgetpu_compiler edgetpu_compiler xray_mobilenetv2_quant_model.tflite"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "authorship_tag": "ABX9TyPPqZvN6xMTNVszXxXeYJ90",
   "collapsed_sections": [],
   "name": "Pneumonia Detection on Edge TPU.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "“edgetpu_environment”",
   "language": "python",
   "name": "edgetpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
